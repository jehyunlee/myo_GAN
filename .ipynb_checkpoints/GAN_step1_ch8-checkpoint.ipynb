{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/conda/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu101\n",
      "0.8.2+cu101\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DEVICE 0: Tesla V100-PCIE-32GB\n",
      "- Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n",
      "# DEVICE 1: Tesla V100-PCIE-32GB\n",
      "- Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n",
      "# DEVICE 2: Tesla V100-PCIE-32GB\n",
      "- Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"# DEVICE {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"- Memory Usage:\")\n",
    "        print(f\"  Allocated: {round(torch.cuda.memory_allocated(i)/1024**3,1)} GB\")\n",
    "        print(f\"  Cached:    {round(torch.cuda.memory_cached(i)/1024**3,1)} GB\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"# GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Current cuda device:  2\n",
      "using cuda: 2, Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 2 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('# Current cuda device: ', torch.cuda.current_device()) # check\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    print(f\"using cuda: {GPU_NUM}, {torch.cuda.get_device_name(GPU_NUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import zipfile\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# location of the HDF5 package, yours may be under /gan/ not /myo_gan/\n",
    "hdf5_file = './celeba_aligned_small.h5py'\n",
    "\n",
    "# how many of the 202,599 images to extract and package into HDF5\n",
    "total_images = 20000\n",
    "\n",
    "with h5py.File(hdf5_file, 'w') as hf:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with zipfile.ZipFile('celeba/img_align_celeba.zip', 'r') as zf:\n",
    "      for i in zf.namelist():\n",
    "        if (i[-4:] == '.jpg'):\n",
    "          # extract image\n",
    "          ofile = zf.extract(i)\n",
    "          img = imageio.imread(ofile)\n",
    "          os.remove(ofile)\n",
    "\n",
    "          # add image data to HDF5 file with new name\n",
    "          hf.create_dataset('img_align_celeba/'+str(count)+'.jpg', data=img, compression=\"gzip\", compression_opts=9)\n",
    "          \n",
    "          count = count + 1\n",
    "          if (count%1000 == 0):\n",
    "            print(\"images done .. \", count)\n",
    "            pass\n",
    "            \n",
    "          # stop when total_images reached\n",
    "          if (count == total_images):\n",
    "            break\n",
    "          pass\n",
    "\n",
    "        pass\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF5\n",
    "\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
